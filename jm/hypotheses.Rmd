---
title: 'CS123 Wikipedia Project: Writeup'
author: "Julian McClellan, Bobby Adsumilli, Andy Zhu"
date: "May 30, 2016"
output:
  pdf_document:
    toc: no
  html_document:
    keep_md: yes
    toc: yes
  word_document:
    toc: yes
---

# Data Files

## wikidata/wikistats (650G Compressed | 2.5TB Uncompressed)

Contains hourly wikipedia article traffic statistics dataset covering 16 month 
period from October 01 2008 to February 6, 2010, from raw anonymous logs 
provided by Domas Mituzas at http://dammit.lt/2007/12/10/wikipedia-page-counters/

Each log file is named with the date and time of collection:

*pagecounts-20090430-230000.gz*

Each line has 4 fields: projectcode, pagename, pageviews, bytes

    en Barack_Obama 997 123091092
    en Barack_Obama%27s_first_100_days 8 850127
    en Barack_Obama,_Jr 1 144103
    en Barack_Obama,_Sr. 37 938821
    en Barack_Obama_%22HOPE%22_poster 4 81005
    en Barack_Obama_%22Hope%22_poster 5 102081


## wikidata/wikilinks (1.1G)

Contains a wikipedia linkgraph dataset provided by Henry Haselgrove.
These files contain all links between *proper english language Wikipedia pages*, 
that is pages in "namespace 0". This includes disambiguation pages and redirect 
pages.

In links-simple-sorted.txt, there is one line for each page that has links from
it. The format of the lines is:

    from1: to11 to12 to13 ...
    from2: to21 to22 to23 ...
    ...
    
where from1 is an integer labelling a page that has links from it, and 
to11 to12 to13 ... are integers labelling all the pages that the page links to. To find the page title that corresponds to integer n, just look up the n-th line in the file *titles-sorted.txt*. 

***

## File summary:

*pagecounts-<date and time>.gz*  
* Contains n lines with 4 fields: projectcode (we want "en" only!), pagename,                     pageviews, and bytes  
* Dates range from 10/01/2008 to 02/06/2010  

*links-simple-sorted.txt*  
* Contains all links between proper english language wikipedia pages.

*titles-sorted.txt*  
* Contains the names of all English Language Wikipedia pages.
    
# Hypothesis

## We will attempt to quantify the relationship between pages and inlinks to these pages.

Imagine the simplest case.  A  page only has one inlink, IL. We propose a linear model for the traffic of that page as:
        
$traf\_page_{t}=\beta_{0}+\beta_{1}traf\_IL_{t}+\beta_{2}bratio\_IL{}_{t}+\beta_{3}bytes\_page_{t}\epsilon_{t}$ 

Where $bratio\_IL{}_{t}=\frac{bytes\_IL_{t}}{bytes\_page_{t}}$.  That is, the bytes ratio is the ratio of the page size in bytes, of the inlink IL to the page.  

We would be most interested in the coefficients $\beta_{1}$ and $\beta_{2}$.  Respectively, these can be interpreted as the change in page traffic when the inlink traffic increases by one unit holding the bytes ratio constant ($\beta_{1}$) and vice versa ($\beta_{2}$).  

However, we do not simply believe that the traffic of a page is a function of the attributes related to the link (or links) to that page.  Given the low dimensions of our data set we use the predictor bytes_page, the size of the page in bytes as an "internal" cause of the traffic to that page.  Bytes can be thought of as a direct way to measure the amount of content the page contains.  

Once the actual data has been more thoroughly explored, we might be inclined to introduce an interaction term between the bytes ratio and inlink traffic.  Doing so complicates a simple explanation of what the regression coefficients mean, but suffice to say, a statistically significant interaction term means that the effect on page traffic that the traffic to the inlink IL has depends on the value of the bytes ratio.

***
Now, to generalize the model, say a page has $i$ inlinks $(2\leq i<\infty)$, then the model generalizes to:  

<center>$traf\;page_{t}=\beta_{0}+\beta_{1}traf\_1+...+\beta_{i}(traf\_i)+\beta_{i+1}bratio\_1+...+\beta_{2i}bratio\_i+\beta_{2i+1}bytes\_page+\epsilon_{t}$</center>


In this case, we would be interested in all $\beta$'s with subscripts greater than 0.  With this linear regression we can test a number of hypotheses:  

Does the traffic from each inlink affect the page traffic in the same way?

Null | $\beta_{1}=\beta_{2}=...=\beta_{i}$

Do the bytes ratios have any effect on page traffic?  

Null | $\beta_{i+1}=\beta_{i+2}=...=\beta_{2i}=0$

Do the bytes ratios all have the same effect?

Null | $\beta_{i+1}=\beta_{i+2}=...=\beta_{2i}$

etc.  

# Procedure

## Initial Setup and Data Filtering

Before the data could even be used, it had to be moved from the read only volume it originated from on Amazon.

Over the course of a week or so, the pagecounts data was copied to a new volume with more permissions, decompressed within that volume, and then uploaded to an S3 bucket to facilitate mrjob use.

However, within the 2.5TB of uncompressed data in the S3 bucket there was a lot of unnecessary information.  The data contained within wikidata/wikilinks pertained only to English language Wikipedia pages, so the first step was to scrub the data clean of anything else.  This process, which took about a day, involved creating 20 instances, each utilizing the s3fs library to mount the S3 bucket onto these instances and running a sed script on each machine to clean data pertaining to a certain hour of the data, and then mopping up the remaining hours with 4 of the 20 instances.  This process reduced the uncompressed size of the data from 2.5TB to 660GB uncompressed.

Our data spanned 16 months, but given our time constraints level of proficiency with methods like mrjob we opted to focus on a smaller subset of data: one out of the 16 months provided to us (42.5G)

This initial setup and exploration of the data was done in an ad-hoc fashion while other group members focused on gaining a familiarity with mrjob and refining our initial rough hypothesis.

## Reconfiguration, Exploration, and Further Filtering of the Data [Bobby feel free to change the title]

Even with our data free of all non-English entries.  There were still a number of entries within the data concerning Wikipedia pages we had no interest in. 

[Bobby will describe the sorts of pages and in general what his mrjob step 1 file tried to accomplish]

## Producing Data to Test Our Hypothesis

Ideally, if we wanted to test our hypothesis on "Page A" over a period of time, we would want to be able to record a single observation for every hour, with each observation containing the traffic and bytes information for Page A *and* it's inlinks.  Unfortunately our data was only a sample of hourly Wikipedia traffic, so we weren't guaranteed to have information about Page A for any particular hour.  (Indeed, in the initial filtering of the non-English entries there were entire files containing no English entries whatsoever!)  Given this fact and our one month data constraint (744 hours), testing our hypothesis on any single page with a large number of inlinks was not feasible, as the higher the number of inlinks a page had, the fewer "complete" observations we would be able to skim from the data.  We chose to focus on pages with anywhere from 1-5 inlinks so that if we were 











      
      




